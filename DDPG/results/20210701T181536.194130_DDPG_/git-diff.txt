diff --git a/.gitignore b/.gitignore
index d73d8c5..28f40b8 100644
--- a/.gitignore
+++ b/.gitignore
@@ -1,3 +1,5 @@
 .idea/
 deeplearning/Recommend/graph_embedding/word2vec/data/*.txt
-deeplearning/Recommend/graph_embedding/word2vec/result/*.txt
\ No newline at end of file
+deeplearning/Recommend/graph_embedding/word2vec/result/*.txt
+*.rar
+*.zip
\ No newline at end of file
diff --git a/deeplearning/RL/DDPG/ddpg_tf2.0.py b/deeplearning/RL/DDPG/ddpg_tf2.0.py
index e69de29..f875923 100644
--- a/deeplearning/RL/DDPG/ddpg_tf2.0.py
+++ b/deeplearning/RL/DDPG/ddpg_tf2.0.py
@@ -0,0 +1,279 @@
+"""A commented Tensorflow 2.0 Keras implementation of DDPG for open AI gym continuous environments.
+This implementation of Deep Deterministic Policy Gradient is different from other implementations
+only in that regard, that I kept to keras only style, meaning that also the somewhat complicated
+loss function is implemented by keras type custom loss mehtods.
+I did this for learning and undestanding only. It is most closely to the OpenAi Spinning up explanation
+and thier implementation:
+https://spinningup.openai.com/en/latest/algorithms/ddpg.html
+https://github.com/openai/spinningup/blob/master/spinup/algos/ddpg/ddpg.py
+TODO: Currently it does not apear to converge in any of the tested envs but did not test for long.
+Could be only HP optimization problem but can't gurantee.
+"""
+
+import numpy as np
+import matplotlib.pyplot as plt
+import sys
+
+import tensorflow as tf
+import tensorflow.keras.layers as KL
+import tensorflow.keras.optimizers as KO
+import tensorflow.keras as K
+
+tf.config.experimental_run_functions_eagerly(True)
+class Memory:
+    """A FIFO experiene replay buffer.
+    """
+
+    def __init__(self, obs_dim, act_dim, size):
+        self.states = np.zeros([size, obs_dim], dtype=np.float32)
+        self.actions = np.zeros([size, act_dim], dtype=np.float32)
+        self.rewards = np.zeros(size, dtype=np.float32)
+        self.next_states = np.zeros([size, obs_dim], dtype=np.float32)
+        self.dones = np.zeros(size, dtype=np.float32)
+        self.ptr, self.size, self.max_size = 0, 0, size
+
+    def store(self, state, action, reward, next_state, done):
+        self.states[self.ptr] = state
+        self.next_states[self.ptr] = next_state
+        self.actions[self.ptr] = action
+        self.rewards[self.ptr] = reward
+        self.dones[self.ptr] = done
+        self.ptr = (self.ptr + 1) % self.max_size
+        self.size = min(self.size + 1, self.max_size)
+
+    def get_sample(self, batch_size=32):
+        idxs = np.random.randint(0, self.size, size=batch_size)
+        return self.states[idxs], self.actions[idxs], self.rewards[idxs], self.next_states[idxs], self.dones[idxs]
+
+
+class DDPGAgent:
+    """The DDPG Agent object.
+    The actor and critic are both created in here.
+    Requires some intel form the env to be creates.
+    Public Methods to interact with:
+        :store: store an experience pair, aka trajectory, aka SARS'D
+        :get_action: get an action from the current actor
+        :get_target_action: get an action from the current target actor - ideally the optimal policy
+        :train: call a training run of the network. Will select a randsom batch and uptadte the neural nets and perform a soft update of target -actor and -critic
+    Params:
+        :state_dim: dimension from the observation space of the environment.
+        :action_n: number of actions. Here, it is implicitly expected to work in continous environments as DDPG was designed for.
+        :act_limit: min and max of the number generates by the network output. Usually 1.
+    """
+
+    def __init__(self, state_dim, action_n, act_limit):
+        # env intel
+        self.action_n = action_n
+        self.state_dim = state_dim
+        self.state_n = state_dim[0]
+        # constants
+        self.ACT_LIMIT = act_limit  # requiered for clipping prediciton aciton
+        self.GAMMA = 0.99  # discounted reward factor
+        self.TAU = 0.005  # soft update factor
+        self.BUFFER_SIZE = int(1e6)  # total replay buffer size. Should be quite large.
+        self.BATCH_SIZE = 100  # training batch size.
+        self.ACT_NOISE_SCALE = 0.1  # the noise is for exploration. simple randomnormal is used here insead of OUNoise like in the orgininal paper an din many examples. OUNoise does not bring much benefit.
+        # create networks
+        self.actor = self._gen_actor_network()  # the local actor wich is trained on.
+        self.actor_target = self._gen_actor_network()  # the target actor which is slowly updated toward optimum
+        self.critic = self._gen_critic_network()
+        self.critic_target = self._gen_critic_network()
+        # Other vital elements
+        self.memory = Memory(self.state_n, self.action_n, self.BUFFER_SIZE)
+        # Dummies. These are required due to the model declartion and loss declarion
+        # style of keras. In short, for complex losses extra inputs need to be declared
+        # but for a model.predict call, they obvisually don't play a role so dummies are passed
+        self.dummy_Q_target_prediction_input = np.zeros((self.BATCH_SIZE, 1))
+        self.dummy_dones_input = np.zeros((self.BATCH_SIZE, 1))
+
+    """-----------------------------ACTOR declarions and methods------------------------------------
+    The Actor takes a state and predicts an action
+    """
+
+    def _gen_actor_network(self):
+        state_input = KL.Input(shape=self.state_dim)
+        dense = KL.Dense(400, activation='relu')(state_input)
+        dense = KL.Dense(300, activation='relu')(dense)
+        out = KL.Dense(self.action_n, activation='tanh')(dense)
+        model = K.Model(inputs=state_input, outputs=out)
+        model.compile(optimizer='adam', loss=self._ddpg_actor_loss)
+        model.summary()
+        return model
+
+    def _ddpg_actor_loss(self, y_true, y_pred):
+        # y_true is Q_prediction = Q_critic_predicted(s,a_actor_predicted)
+        return -K.backend.mean(y_true)
+
+    def get_action(self, states, noise=None):
+        """Returns an action (=prediction of local actor) given a state.
+        Adds a gaussion noise for exploration.
+        params:
+            :state: the state batch
+            :noise: add noise. If None defaults self.ACT_NOISE_SCALE is used.
+                    If 0 ist passed, no noise is added and clipping passed
+        """
+        if noise is None: noise = self.ACT_NOISE_SCALE
+        if len(states.shape) == 1: states = states.reshape(1, -1)
+        action = self.actor.predict_on_batch(states)
+        if noise != 0:
+            action += noise * np.random.randn(self.action_n)
+            action = np.clip(action, -self.ACT_LIMIT, self.ACT_LIMIT)
+        return action
+
+    def get_target_action(self, states):
+        return self.actor_target.predict_on_batch(states)
+
+    def train_actor(self, states, actions):
+        # Q_predictions can not be calculated in keras loss function because it depends on the prediction
+        # of another model (critic) with the prediction of this model (actions) as a parameter.
+        actions_predict = self.get_action(states, noise=0)
+        Q_predictions = self.get_Q(states, actions_predict)
+        self.actor.train_on_batch(states, Q_predictions)
+
+    """-----------------------------CRITIC declarion and methods------------------------------------
+    The Critic, in the DDPG case, is the Q(s,a)
+    """
+
+    def _gen_critic_network(self):
+        # Inputs to network. Most of them are for the loss function, not for the feed forward
+        state_input = KL.Input(shape=self.state_dim, name='state_input')
+        action_input = KL.Input(shape=(self.action_n,), name='action_input')
+        Q_target_prediction_input = KL.Input(shape=(1,), name='Q_target_prediction_input')
+        dones_input = KL.Input(shape=(1,), name='dones_input')
+        # define network structure
+        concat_state_action = KL.concatenate([state_input, action_input])
+        dense = KL.Dense(400, activation='relu')(concat_state_action)
+        dense = KL.Dense(300, activation='relu')(dense)
+        out = KL.Dense(1, activation='linear')(dense)
+        model = K.Model(inputs=[state_input, action_input, Q_target_prediction_input, dones_input], outputs=out)
+        model.compile(optimizer='adam', loss=self._ddpg_critic_loss(Q_target_prediction_input, dones_input))
+        model.summary()
+        return model
+
+    def _ddpg_critic_loss(self, Q_target_prediction_input, dones_input):
+        def loss(y_true, y_pred):
+            # remember: y_true = rewards ; y_pred = Q
+            target_Q = y_true + (self.GAMMA * Q_target_prediction_input * (1 - dones_input))
+            mse = K.losses.mse(target_Q, y_pred)
+            return mse
+
+        return loss
+
+    def train_critic(self, states, next_states, actions, rewards, dones):
+        """Train the critic using a trajectory batch from memory.
+        This is part of the core algorithm. https://spinningup.openai.com/en/latest/algorithms/ddpg.html
+        The loss of the critic requires the predicted Q(s,a) which is the prediction of the critic.
+        """
+        next_actions = self.get_target_action(next_states)
+        Q_target_prediction = self.get_target_Q(next_states, next_actions)
+        self.critic.train_on_batch([states, actions, Q_target_prediction, dones], rewards)
+
+    def get_Q(self, states, actions):
+        return self.critic.predict([states, actions, self.dummy_Q_target_prediction_input, self.dummy_dones_input])
+
+    def get_target_Q(self, states, actions):
+        return self.critic_target.predict_on_batch(
+            [states, actions, self.dummy_Q_target_prediction_input, self.dummy_dones_input])
+
+    """-----------------------------AGENT interface and logic------------------------------------
+    The Critic, in the DDPG case, is the Q(s,a)
+    """
+
+    def _soft_update_actor_and_critic(self):
+        """Makes a soft update of the target models with the latest local model weights.
+        Uses the factor self.TAU to determine the how soft. Usually, tau is small.
+        """
+        # Critic soft update:
+        weights_critic_local = np.array(self.critic.get_weights())
+        weights_critic_target = np.array(self.critic_target.get_weights())
+        self.critic.set_weights(self.TAU * weights_critic_local + (1.0 - self.TAU) * weights_critic_target)
+        # Actor soft update
+        weights_actor_local = np.array(self.actor.get_weights())
+        weights_actor_target = np.array(self.actor_target.get_weights())
+        self.actor_target.set_weights(self.TAU * weights_actor_local + (1.0 - self.TAU) * weights_actor_target)
+
+    def store(self, state, action, reward, next_state, done):
+        """Stores a trajectory
+        Just passes though to memory to keep object structure.
+        """
+        self.memory.store(state, action, reward, next_state, done)
+
+    def train(self):
+        """Trains the networks of the agent (local actor and critic) and soft-updates thier target.
+        """
+        states, actions, rewards, next_states, dones = self.memory.get_sample(batch_size=self.BATCH_SIZE)
+        self.train_critic(states, next_states, actions, rewards, dones)
+        self.train_actor(states, actions)
+        self._soft_update_actor_and_critic()
+
+
+# -----------------------------MAIN------------------------
+# The main is from OpenAI spinning up libary
+import gym
+
+if __name__ == "__main__":
+    GAME = 'HalfCheetah-v2'  # 'BipedalWalker-v2'   #'LunarLanderContinuous-v2'   #'HalfCheetah-v2'
+    GAMMA = 0.99
+    EPOCHS = 1000
+    MAX_EPISODE_LENGTH = 3000
+    START_STEPS = 10000     # 初始步数范围内随机选动作，超过这个步数就采用学到的策略选动作
+    RENDER_EVERY = 10
+
+    env = gym.make(GAME)
+    agent = DDPGAgent(env.observation_space.shape, env.action_space.shape[0], max(env.action_space.high))
+
+    state, reward, done, ep_rew, ep_len, ep_cnt = env.reset(), 0, False, [0.0], 0, 0
+    total_steps = MAX_EPISODE_LENGTH * EPOCHS
+
+    # Main loop: collect experience in env and update/log each epoch
+    for t in range(total_steps):
+        # render from time for time because it floats my boat
+        if ep_cnt % RENDER_EVERY == 0:
+            env.render()
+        # get action, at the beginning randomly later by neural net output+noise
+        if t > START_STEPS:
+            action = agent.get_action(state)
+            action = np.squeeze(action)
+        else:
+            action = env.action_space.sample()
+
+        # Step the env
+        next_state, reward, done, _ = env.step(action)
+        ep_rew[-1] += reward  # keep adding to the last element till done
+        ep_len += 1
+
+        # Ignore the "done" signal if it comes from hitting the time
+        # horizon (that is, when it's an artificial terminal signal
+        # that isn't based on the agent's state)
+        done = False if ep_len == MAX_EPISODE_LENGTH else done
+
+        # Store experience to replay buffer
+        agent.store(state, action, reward, next_state, done)
+
+        # Super critical, easy to overlook step: make sure to update
+        # most recent observation!
+        state = next_state
+
+        if done or (ep_len == MAX_EPISODE_LENGTH):
+            ep_cnt += 1
+            if True:  # ep_cnt % RENDER_EVERY == 0:
+                print(f"Episode: {len(ep_rew) - 1}, Reward: {np.mean(ep_rew[-12:-2])}")
+            ep_rew.append(0.0)
+            """Perform all DDPG updates at the end of the trajectory.
+            Train on a randomly sampled batch as often there were steps in this episode.
+            I don't understand why it is updated that often, this is from the TD3 paper and 
+            in accordance with the openai implementation.
+            """
+            for _ in range(ep_len):
+                agent.train()
+
+            state, reward, done, ep_ret, ep_len = env.reset(), 0, False, 0, 0
+
+    # make simple moving average over 5 episodes (smoothing) and plot
+    SMA_rewards = np.convolve(ep_rew, np.ones((5,)) / 5, mode='valid')
+    # Plot learning curve
+    plt.style.use('seaborn')
+    plt.plot(SMA_rewards)
+    plt.xlabel('Episode')
+    plt.ylabel('Total Reward')
+    plt.show()
diff --git a/deeplearning/RL/DDPG/tf2rl_run_ddpg.py b/deeplearning/RL/DDPG/tf2rl_run_ddpg.py
index e69de29..6998317 100644
--- a/deeplearning/RL/DDPG/tf2rl_run_ddpg.py
+++ b/deeplearning/RL/DDPG/tf2rl_run_ddpg.py
@@ -0,0 +1,21 @@
+import gym
+from tf2rl.algos.ddpg import DDPG
+from tf2rl.experiments.trainer import Trainer
+
+
+parser = Trainer.get_argument()
+parser = DDPG.get_argument(parser)
+args = parser.parse_args()
+
+env = gym.make("HalfCheetah-v2")
+test_env = gym.make("HalfCheetah-v2")
+policy = DDPG(
+    state_shape=env.observation_space.shape,
+    action_dim=env.action_space.high.size,
+    gpu=0,  # Run on CPU. If you want to run on GPU, specify GPU number
+    memory_capacity=10000,
+    max_action=env.action_space.high[0],
+    batch_size=32,
+    n_warmup=500)
+trainer = Trainer(policy, env, args, test_env=test_env)
+trainer()
\ No newline at end of file
diff --git a/deeplearning/RL/MBRL/agent/mb_agent.py b/deeplearning/RL/MBRL/agent/mb_agent.py
index f4fddda..3808185 100644
--- a/deeplearning/RL/MBRL/agent/mb_agent.py
+++ b/deeplearning/RL/MBRL/agent/mb_agent.py
@@ -1,5 +1,7 @@
 """
 定义基于模型的agent子类
+该类的主要功能是封装了环境模型、策略、以及真实环境这几个类实例
+向外提供了一系列接口，使得实现变得简单
 """
 from .base_agent import BaseAgent
 
diff --git a/deeplearning/RL/MBRL/infrastructure/tensorflow_util.py b/deeplearning/RL/MBRL/infrastructure/tensorflow_util.py
index e69de29..06e1371 100644
--- a/deeplearning/RL/MBRL/infrastructure/tensorflow_util.py
+++ b/deeplearning/RL/MBRL/infrastructure/tensorflow_util.py
@@ -0,0 +1,5 @@
+"""
+定义构建神经网络模型所需的tensorflow工具函数
+"""
+
+def build_mlp()
\ No newline at end of file
diff --git a/deeplearning/RL/MBRL/infrastructure/utils.py b/deeplearning/RL/MBRL/infrastructure/utils.py
index b8eef96..c21fbf7 100644
--- a/deeplearning/RL/MBRL/infrastructure/utils.py
+++ b/deeplearning/RL/MBRL/infrastructure/utils.py
@@ -102,3 +102,17 @@ def get_path_length(path):
     """
 
     return len(path["reward"])
+
+
+def normalize(data, mean, std, eps=1e-8):
+    """
+    对模型的输入进行标准化
+    """
+    return (data - mean) / (std + eps)
+
+
+def unnormalize(data, mean, std):
+    """
+    解标准化，从标准化数据转到非标准化数据
+    """
+    return data * std + mean
diff --git a/deeplearning/RL/MBRL/model/ff_model.py b/deeplearning/RL/MBRL/model/ff_model.py
index e69de29..c8478de 100644
--- a/deeplearning/RL/MBRL/model/ff_model.py
+++ b/deeplearning/RL/MBRL/model/ff_model.py
@@ -0,0 +1,108 @@
+"""
+定义环境模型，从 (s_t, a_t) -> (s_t+1 - s_t) 的映射
+"""
+
+import tensorflow as tf
+from MBRL.model.base_model import BaseModel
+from MBRL.infrastructure.utils import normalize, unnormalize
+import numpy as np
+
+
+class FFModel(tf.keras.Model, BaseModel):
+    def __init__(self, ac_dim, ob_dim, n_layers, size, learning_rate=0.001):
+        """
+        定义模型层及相关变量
+        Args:
+            ac_dim: 动作空间维度 int
+            ob_dim: 状态空间维度 int
+            n_layers: 隐藏层数量 int
+            size: 隐藏层维度（节点个数） int
+            learning_rate: 学习率 float
+        """
+        super().__init__()
+
+        self.ac_dim = ac_dim
+        self.ob_dim = ob_dim
+        self.n_layers = n_layers
+        self.size = size
+        self.learning_rate = learning_rate
+
+        # 构建多层感知机神经网络模型
+        self.input_layer = tf.keras.layers.Dense(self.ac_dim + self.ob_dim, activation=tf.nn.tanh)
+        self.hidden_layers = []
+        for _ in range(self.n_layers - 1):
+            self.hidden_layers.append(tf.keras.layers.Dense(self.size, activation=tf.nn.tanh))
+        self.output_layer = tf.keras.layers.Dense(self.ob_dim, activation=tf.keras.activations.linear)  # 激活函数不做改变
+
+        # 定义优化器
+        self.optimizer = tf.keras.optimizers.Adam(
+            learning_rate=self.learning_rate
+        )
+
+        # 定义模型输入和输出变量的均值和标准差，用于标准化
+        self.obs_mean = None
+        self.obs_std = None
+        self.acs_mean = None
+        self.acs_std = None
+        self.delta_mean = None
+        self.delta_std = None
+
+    def call(
+            self,
+            obs_unnormalized,
+            acs_unnormalized,
+            obs_mean,
+            obs_std,
+            acs_mean,
+            acs_std,
+            delta_mean,
+            delta_std,
+    ):
+        """
+        定义模型前馈过程
+        """
+        # 正则化输入数据
+        obs_normalized = normalize(obs_unnormalized, obs_mean, obs_std)
+        acs_normalized = normalize(acs_unnormalized, acs_mean, acs_std)
+
+        # 连接状态和动作，形成输入维度
+        concatenated_input = np.concatenate((obs_normalized, acs_normalized), axis=1)
+
+        # 定义前馈过程
+        x = self.input_layer(concatenated_input)
+        for hidden in self.hidden_layers:
+            x = hidden(x)
+        delta_pred_normalized = self.output_layer(x)
+
+        # 输出的是(s_t+1 - s_t)，所以要加上输入的状态s_t，才能得到下一个状态值
+        next_obs_pred = obs_unnormalized + unnormalize(delta_pred_normalized, delta_mean, delta_std)
+
+        return next_obs_pred, delta_pred_normalized
+
+    def update(self, observations, actions, next_observations, data_statistics):
+        """
+        定义模型损失和梯度下降更新
+        """
+        # 计算正则化的目标值
+        target = normalize(next_observations - observations, data_statistics['delta_mean'],
+                           data_statistics['delta_std'])
+
+        # 定义训练过程
+        with tf.GradientTape() as tape:
+            # 计算模型预测值
+            _, pred_delta_normalized = self(observations, actions, **data_statistics)
+            loss = tf.keras.losses.sparse_categorical_crossentropy(y_true=target, y_pred=pred_delta_normalized)
+            loss = tf.reduce_mean(loss)
+        grads = tape.gradient(loss, self.variables)
+        self.optimizer.apply_gradients(grads_and_vars=zip(grads, self.variables))
+
+        return {
+            "Training Loss": loss
+        }
+
+    def get_prediction(self, ob_no, ac_na, data_statistics):
+        """
+        定义模型预测输出
+        """
+        pred, _ = self(ob_no, ac_na, **data_statistics)
+        return pred
